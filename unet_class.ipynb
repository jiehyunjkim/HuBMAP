{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import unet\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, List\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from skimage.transform import resize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (512, 512) \n",
    "channels = 1\n",
    "classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/Users/jiehyun/kaggle/\"\n",
    "# define the path to the images and masks dataset\n",
    "IMAGE_DATASET_PATH = DATASET_PATH + \"input/hubmap-organ-segmentation/train_images\"\n",
    "MASK_DATASET_PATH = DATASET_PATH + \"input/hubmap-organ-segmentation/binary_masks\"\n",
    "TRAIN_CSV = DATASET_PATH + \"input/hubmap-organ-segmentation/train.csv\"\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "TOTAL_NUM_DATA = 351\n",
    "OUTPUT_FOLDER = \"/Users/jiehyun/kaggle/output/\"\n",
    "IMG_NPY = OUTPUT_FOLDER + 'img_npy'\n",
    "MASK_NPY = OUTPUT_FOLDER + 'mask_npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(count:int, splits:Tuple[float]=(0.71, 0.145, 0.145), **kwargs) -> List[tf.data.Dataset]:\n",
    "    return [tf.data.Dataset.from_tensor_slices(_build_samples(int(split * count), **kwargs)) for split in splits]\n",
    "\n",
    "\n",
    "def _build_samples(sample_count:int, **kwargs) -> Tuple[np.array, np.array]:\n",
    "    \n",
    "    for i in range(len(train_df['id'])):\n",
    "        idx = random.randint(0, len(train_df) - 1)\n",
    "        img_id = train_df['id'][idx]\n",
    "        loadedimages = np.load(IMG_NPY + f'/{img_id}.npy', allow_pickle=True).copy()\n",
    "        loadedmasks = np.load(MASK_NPY + f'/{img_id}.npy', allow_pickle=True).copy()\n",
    "\n",
    "        output_shape = (512, 512)\n",
    "        loadedimages = resize(loadedimages, output_shape)\n",
    "        loadedmasks = resize(loadedmasks, output_shape)\n",
    "    \n",
    "    # now let's go to numpyland\n",
    "    images = np.empty((sample_count, IMAGE_SIZE[0], IMAGE_SIZE[1], 1))\n",
    "    labels = np.empty((sample_count, IMAGE_SIZE[0], IMAGE_SIZE[1], 2))\n",
    "    \n",
    "    for i in range(sample_count):\n",
    "        #image, mask = loadedimages[i], loadedmasks[i]\n",
    "        image, mask = loadedimages, loadedmasks\n",
    "\n",
    "        image = image.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3)).astype(np.float)\n",
    "        mask = mask.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 1)) #.astype(np.float)\n",
    "\n",
    "        image = tf.cast(image, tf.float32)/255.0\n",
    "        mask -= 1\n",
    "        #\n",
    "        # Use Tensorflow to flip the image horizontally\n",
    "        #\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            image = tf.image.flip_left_right(image)\n",
    "            mask = tf.image.flip_left_right(mask)\n",
    "        #\n",
    "        # Use Tensorflow to flip the image vertically\n",
    "        #\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            image = tf.image.flip_up_down(image)\n",
    "            mask = tf.image.flip_up_down(mask)\n",
    "        #\n",
    "        # Use Tensorflow to rotate the image 90 degrees\n",
    "        #\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            image = tf.image.rot90(image, k=1, name=None)\n",
    "            mask = tf.image.rot90(mask, k=1, name=None)\n",
    "        \n",
    "        # augmentation done, let's store the image\n",
    "        #images[i] = image\n",
    "        images = image\n",
    "\n",
    "        # here we split the mask to background and foreground\n",
    "        fg = np.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], 1), dtype=np.bool)\n",
    "        fg[mask == 255] = 1\n",
    "        bg = np.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], 1), dtype=np.bool)\n",
    "        bg[mask == 0] = 1\n",
    "        \n",
    "        labels[i, :, :, 0] = bg[:,:,0]\n",
    "        labels[i, :, :, 1] = fg[:,:,0]\n",
    "\n",
    "        #images.flags.writeable = True\n",
    "        #labels.flags.writeable = True\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w2/kjqd2k2x70z1qwx2jy2y7j580000gn/T/ipykernel_45536/4156737334.py:25: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  image = image.reshape((IMAGE_SIZE[0], IMAGE_SIZE[1], 3)).astype(np.float)\n",
      "/var/folders/w2/kjqd2k2x70z1qwx2jy2y7j580000gn/T/ipykernel_45536/4156737334.py:54: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fg = np.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], 1), dtype=np.bool)\n",
      "/var/folders/w2/kjqd2k2x70z1qwx2jy2y7j580000gn/T/ipykernel_45536/4156737334.py:56: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  bg = np.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], 1), dtype=np.bool)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions 512 and 249 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [129], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train, val, test \u001b[38;5;241m=\u001b[39m load_data(TOTAL_NUM_DATA, splits\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.145\u001b[39m, \u001b[38;5;241m0.145\u001b[39m))\n",
      "Cell \u001b[0;32mIn [128], line 2\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(count, splits, **kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(count:\u001b[38;5;28mint\u001b[39m, splits:Tuple[\u001b[38;5;28mfloat\u001b[39m]\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.145\u001b[39m, \u001b[38;5;241m0.145\u001b[39m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset]:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(_build_samples(\u001b[38;5;28mint\u001b[39m(split \u001b[38;5;241m*\u001b[39m count), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)) \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits]\n",
      "Cell \u001b[0;32mIn [128], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(count:\u001b[38;5;28mint\u001b[39m, splits:Tuple[\u001b[38;5;28mfloat\u001b[39m]\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.145\u001b[39m, \u001b[38;5;241m0.145\u001b[39m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset]:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_build_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:814\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    737\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    738\u001b[0m   \u001b[39m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \n\u001b[1;32m    740\u001b[0m \u001b[39m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:4720\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m   4717\u001b[0m batch_dim \u001b[39m=\u001b[39m tensor_shape\u001b[39m.\u001b[39mDimension(\n\u001b[1;32m   4718\u001b[0m     tensor_shape\u001b[39m.\u001b[39mdimension_value(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_shape()[\u001b[39m0\u001b[39m]))\n\u001b[1;32m   4719\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m-> 4720\u001b[0m   batch_dim\u001b[39m.\u001b[39;49massert_is_compatible_with(\n\u001b[1;32m   4721\u001b[0m       tensor_shape\u001b[39m.\u001b[39;49mDimension(\n\u001b[1;32m   4722\u001b[0m           tensor_shape\u001b[39m.\u001b[39;49mdimension_value(t\u001b[39m.\u001b[39;49mget_shape()[\u001b[39m0\u001b[39;49m])))\n\u001b[1;32m   4724\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mtensor_slice_dataset(\n\u001b[1;32m   4725\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors,\n\u001b[1;32m   4726\u001b[0m     output_shapes\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_shapes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure),\n\u001b[1;32m   4727\u001b[0m     is_files\u001b[39m=\u001b[39mis_files,\n\u001b[1;32m   4728\u001b[0m     metadata\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39mSerializeToString())\n\u001b[1;32m   4729\u001b[0m \u001b[39msuper\u001b[39m(TensorSliceDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/framework/tensor_shape.py:299\u001b[0m, in \u001b[0;36mDimension.assert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39m\"\"\"Raises an exception if `other` is not compatible with this Dimension.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m    is_compatible_with).\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_compatible_with(other):\n\u001b[0;32m--> 299\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDimensions \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m are not compatible\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    300\u001b[0m                    (\u001b[39mself\u001b[39m, other))\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 512 and 249 are not compatible"
     ]
    }
   ],
   "source": [
    "train, val, test = load_data(TOTAL_NUM_DATA, splits=(0.71, 0.145, 0.145))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "unet_model = unet.build_model(channels=channels,\n",
    "                              num_classes=classes,\n",
    "                              layer_depth=5,\n",
    "                              filters_root=64,\n",
    "                              padding=\"same\")\n",
    "\n",
    "unet.finalize_model(unet_model, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = unet.Trainer(checkpoint_callback=False,\n",
    "                       learning_rate_scheduler=unet.SchedulerType.WARMUP_LINEAR_DECAY,\n",
    "                       warmup_proportion=0.1,\n",
    "                       learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(unet_model,\n",
    "            train,\n",
    "            val,\n",
    "            epochs=50,\n",
    "            batch_size=10\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 10\n",
    "fig, axs = plt.subplots(rows, 3, figsize=(8, 30))\n",
    "for ax, (image, label) in zip(axs, test.take(rows).batch(1)):\n",
    "  \n",
    "  prediction = unet_model.predict(image)\n",
    "  ax[0].matshow(image[0, :, :, 0])\n",
    "  ax[1].matshow(label[0, :, :, 1], cmap=\"gray\")\n",
    "  ax[2].matshow(prediction[0].argmax(axis=-1), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unet_model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
